---
title: "Assignment_5"
author: "Nicolas Mollier"
date: "2/2/2021"
output: html_document
---

# Declaration and Project Repository

For this assignment, I worked together with Max Knei√üler (Student ID: 4121846). The link to my project repository is https://github.com/nicolasmollier/DSPM_Assignment_5

```{r}
rm(list = ls())
```


```{r Working Directory, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "/home/nicolas/Desktop/Uni/WiSe20_21/DS400_DS_Project_Management/Assignments/Assignment_5")
setwd("/home/nicolas/Desktop/Uni/WiSe20_21/DS400_DS_Project_Management/Assignments/Assignment_5")
```

```{r Packages}
library(tidyverse)
library(knitr)
library(jsonlite)
library(httr)
library(rlist)
```

# Task 2

The default quota is 5000 API calls per day and rate limitation of 5 requests per second. Therefore, I originally used `Sys.Sleep(1/5)` in order to make sure that the rate limitation of 5 requests per second is not violated. Theoretically, with `Sys.Sleep(1/5)` in place between each request to the ticketmaster API, it is not possible to make more than 5 requests per second even if the pure execution time of the code that performs the request was virtually zero. Since using `Sys.Sleep(1/5)` repeatedly caused "Spike arrest violations", I instead used `Sys.Sleep(2/5)` which seemed to reduce the frequency of the occurrence of that error. In the section "Discovery API" subsection "Venue Search", query parameters that can be used are listed. The query parameters that we are going to use are `locale`, `size`, `page` and `countryCode`. 

Before we can start making requests in Task 3, we need to make the API key -that we retrieved from the API Explorer and saved in the file `api_key.R`- available in our script as `tm_api_key`.

```{r API key}
source("api_key.R")
```

# Task 3

First of all, I define the two variables `country_code` and `page_size` for Task 3. `page_size` can be used inside API requests for the `size` parameter which enables me to extract 100 cases per request instead of 20 (default). Defining the `country_code` variable enables me to quickly adapt the code for request of venues in other countries.

```{r}
country_code <- "DE"
page_size <- 100
```

For the first GET request, we use the `tm_api_key` as `apikey`, `country_code` set to "DE" as `ountryCode` value and `page_size` set to 100 as value for the `size` parameter. Additionally, we have to set `locale = '*'`. Otherwise, we do not get all German venues that are available and almost all longitude and latitude values are missing. Setting `locale = '*'` resolves these issues. As described we have to add /discovery/v2/venues to the root url https://app.ticketmaster.com/discovery/v2/ when performing venue searches. `Sys.Sleep(2/5)` is used after the request to make sure that we adhere to the rate limit of 5 requests per second.

```{r GET request 1}
response <- GET(url = "https://app.ticketmaster.com/discovery/v2/venues",
    query = list(apikey = tm_api_key,
                 countryCode = country_code,
                 size = page_size,
                 locale = '*'))
Sys.sleep(2/5)
```

The functions `content()` and `fromJSON()` are used sequentially to retrieve the content that we are interested in. The resulting variable `content` is a list of three lists. One of the lists is named `_embedded`. `_embedded` contains another list called `venues`. This is where the variables that we are looking for are saved. Note, that some of the variables (`name`, `postalCode`, `url`) are vectors and can immediately be retrieved. The variables `city` and `address` are lists and the variables `longitude` and `latitude` are character variables inside a list called `location`. In order to create a data frame containing all those variables, we first access the list `_embedded` and `venues` inside the `content` object and save the result as `venues_df`. From this data frame, we only select the seven columns of interest to us and save the resulting data frame as `venues_select`. `venues_select` is a data frame where some of the variables of interest are themselves data frames due to the fact that inside the list `venues` (inside `content`) the variables `city` and `address` are lists and the variables `longitude` and `latitude` are character variables inside a list called `location`.

```{r}
content_json <- content(response, as = "text")
content <- fromJSON(content_json)
venues_df <-content[["_embedded"]][["venues"]]
venues_select <- venues_df %>% 
  select(name, city, postalCode, address, url, location)
glimpse(venues_select)
```

The vector variables `name`, `postalCode` and `url` can be selected directly.

```{r}
venue_data <- venues_select %>% 
  select(is.vector)
```

The other variables `ity`, `address`, `longitude` and `latitude` have to be selected by first accessing the respective column that is itself a data frame and then accessing the respective column inside that data frame. The resulting vectors are saved as columns in `venue_data`. `venue_data` has 100 rows and 7 columns. 

```{r}
venue_data["city"] <- venues_select$city[["name"]]
venue_data["address"] <- venues_select$address[["line1"]]
venue_data["longitude"] <- venues_select$location["longitude"]
venue_data["latitude"] <- venues_select$location["latitude"]


venue_data %>% 
  glimpse()
```


# Task 4

A look at the list element `page` in the object `content` reveals that we extracted 100 venues out of 12238 from 123 pages with at most 100 venues each. We can extract the venues from all the pages available by looping through them. In order to do this, we write a for loop that starts with the query parameter `page` set to 0 and increases the value after each iteration until we reach the last page. Since the first page has the index 0, we need to start the iteration through all pages at `page = 0`. The parameter `locale` is set to `'*'` to make sure that we retrieve all German venues.


We start by extracting the total number of pages for Germany and save it as `total_pages`. The total number of venues (elements) is saved  as `n`. After that, two data frames are created. `venue_data_complete` will hold all venues from all the pages of venues in Germany. Therefore, we initialize it with variables (columns) of lengths `n`. The data frame `venue_data` is just a temporary data frame to hold the elements of the respective page in the respective iteration of the loop.

```{r}
total_pages <- content$page$totalPages
n <- content$page$totalElements

venue_data_complete <-
  data.frame(
    name = character(n),
    city = character(n),
    postalCode = character(n),
    address = character(n),
    url = character(n),
    longitude = numeric(n),
    latitude = numeric(n),
    stringsAsFactors = FALSE
  )
venue_data <-
  data.frame(
    name = character(page_size),
    city = character(page_size),
    postalCode = character(page_size),
    address = character(page_size),
    url = character(page_size),
    longitude = numeric(page_size),
    latitude = numeric(page_size),
    stringsAsFactors = FALSE
  )
```

Overall, there are `r n` venues to be retrieved from `r total_pages`. We start the iteration at page 0 and end it at page `r total_pages - 1`. In each iteration, we use the same GET request that we used above, except that now the parameter `page` is set to the respective page in each iteration. The extraction of the variables of interest from the `response` object is done in the same way as well. The if statement is needed to accommodate the fact that the last page generally does not have 100 but less elements. As long as we are not at the last page, we obtain a temporary data frame called `venue_data` with 100 rows that contains the seven variables. For the last page, the initialization of this temporary data frame `venue_data` is different, since the number of rows has to be adapted to the number of cases in the last page. We can get the necessary row number by calculating the remainder of the division of the total number of elements and the page size (here: 100) (`n %% page_size`). The result of this loop is a data frame called `venue_data_complete` that contains every German venue. Note, that I used `Sys.sleep(2/5)` instead of `Sys.sleep(2/5)`. Theoretically, with `Sys.Sleep(1/5)` in place between each request to the ticketmaster API, it is not possible to make more than 5 requests per second. However, using `Sys.Sleep(1/5)` repeatedly prompted "Spike arrest violations" when running the loop for the German venues. Using `Sys.sleep(2/5)` seems to reduce the frequency of facing such an error but it still happened from time to time, especially when I ran the code of my entire script more than once per day.

```{r}
for(i in 0:(total_pages-1)){
  response <- GET(url = "https://app.ticketmaster.com/discovery/v2/venues",
    query = list(apikey = tm_api_key,
                 countryCode = country_code,
                 size = page_size,
                 page = i,
                 locale = '*'))
  content_json <- content(response, as = "text")
  content <- fromJSON(content_json)
  content_embedded <- content[["_embedded"]]
  # assign(paste0("venues",i), content_embedded[["venues"]])
  venues <- content_embedded[["venues"]]
  # as long as we have not reached the last page, we insert data frames 
  # with 100 cases into venue_data_complete
  if(i < (total_pages-1)){
    venue_data["name"] <- venues[["name"]]
    venue_data["postalCode"] <- venues[["postalCode"]]
    venue_data["url"] <- venues[["url"]]
    venue_data["city"] <- venues$city[["name"]]
    venue_data["address"] <- venues$address[["line1"]]
    venue_data["longitude"] <- as.double(venues$location[["longitude"]])
    venue_data["latitude"] <- as.double(venues$location[["latitude"]])
    venue_data <- venue_data %>% 
      select(name, city, postalCode, address, url, longitude, latitude)
    # put the content of the temporary data frame in the right spot in venue_data_complete 
    venue_data_complete[(i*page_size + 1) : ((i+1) * page_size), ] <- venue_data
    # print(paste("Page:", i+1, "| Progress:", scales::percent(i/total_pages), sep = " "))
    Sys.sleep(2/5)
  # as soon as we reach the last page, the number of rows to be inserted into
  # venue_data_complete reduces to n %% 100 (the remainder of the division of
  # the total number of elements and the page size)
  } else{
    venue_data <- data.frame(
      name = character(n%%page_size),
      city = character(n%%page_size),
      postalCode = character(n%%page_size),
      address = character(n%%page_size),
      url = character(n%%page_size),
      longitude = numeric(n%%page_size),
      latitude = numeric(n%%page_size),
      stringsAsFactors = FALSE
      )
    
    venue_data["name"] <- venues[["name"]]
    venue_data["postalCode"] <- venues[["postalCode"]]
    venue_data["url"] <- venues[["url"]]
    venue_data["city"] <- venues$city[["name"]]
    venue_data["address"] <- venues$address[["line1"]]
    venue_data["longitude"] <- as.double(venues$location[["longitude"]])
    venue_data["latitude"] <- as.double(venues$location[["latitude"]])
    venue_data <- venue_data %>% 
      select(name, city, postalCode, address, url, longitude, latitude)
    # put the content of the temporary data frame for the last page in the right spot in venue_data_complete 
    venue_data_complete[(i*page_size + 1) : (i*page_size + n%%page_size), ] <- venue_data
    # print("final iteration")
    Sys.sleep(2/5)
  }
}

```

The final data frame containing all German venues has `nrow(venue_data_complete)` rows.

```{r}
glimpse(venue_data_complete)
```


# Task 5) Visualizing the extracted data

Next, let us visualize the locations of the extracted venues. Some coordinates are outside the boundaries of the German borders. Venues with coordinates that are outside the extreme point ranges that were derived from the extreme points for Germany listed on https://en.wikipedia.org/wiki/Geography_of_Germany#Extreme_points are not included in the following map.

```{r}
max_lat_de <- 55.0846
min_lat_de <- 47.271679
min_long_de <- 5.866944
max_long_de <- 15.043611

# keep only the venues with location coordinates inside the area that is defined
# by the extreme points
venue_data_complete <- venue_data_complete %>% 
  filter(longitude < max_long_de, longitude > min_long_de,
         latitude < max_lat_de, latitude > min_lat_de)
```

The venue locations are indicated on the map by points. The opacity of the points was reduced to make locations with a high concentration of venues more visible.

```{r}
ggplot() +
  geom_polygon(
    aes(x = long, y = lat, group = group), data = map_data("world", region = "Germany"),
    fill = "gray90",color = "black") +
  geom_point(data = venue_data_complete, aes(longitude, latitude), 
             color = "royalblue3",
             alpha = 0.5) +
  theme_void() + coord_quickmap() +
  labs(title = "Event locations across Germany", caption = "Source: ticketmaster.com") +
  theme(title = element_text(size=12, face='bold'),
    plot.caption = element_text(face = "italic")) 
```


# Task 6) Event locations in Belgium

We repeat everything that we have done previously for German venues but set the `country_code` to "GB" this time to retrieve venues in Great Britain. Another thing that we have to adapt are the extreme points to filter out those venues with location coordinates that are way outside the country borders of UK. Otherwise, the code is not changed compared to the German case. 

```{r}
country_code <- "GB"
```


```{r GET request 2}

response <- GET(url = "https://app.ticketmaster.com/discovery/v2/venues",
    query = list(apikey = tm_api_key,
                 countryCode = country_code,
                 size = page_size,
                 locale = '*'))
Sys.sleep(2/5)
```


```{r}
content_json <- content(response, as = "text")
content <- fromJSON(content_json)
venues <-content[["_embedded"]]
venues <- venues[["venues"]]
venues_select <- venues %>% 
  select(name, city, postalCode, address, url, location)
glimpse(venues_select)

venue_data <- venues_select %>% 
  select(is.vector)
venue_data["city"] <- venues_select$city[["name"]]
venue_data["address"] <- venues_select$address[["line1"]]
venue_data["longitude"] <- venues_select$location["longitude"]
venue_data["latitude"] <- venues_select$location["latitude"]


glimpse(venue_data)
```


```{r}
total_pages <- content$page$totalPages
n <- content$page$totalElements

venue_data_complete <-
  data.frame(
    name = character(n),
    city = character(n),
    postalCode = character(n),
    address = character(n),
    url = character(n),
    longitude = numeric(n),
    latitude = numeric(n),
    stringsAsFactors = FALSE
  )
venue_data <-
  data.frame(
    name = character(page_size),
    city = character(page_size),
    postalCode = character(page_size),
    address = character(page_size),
    url = character(page_size),
    longitude = numeric(page_size),
    latitude = numeric(page_size),
    stringsAsFactors = FALSE
  )
```

Overall, there are `r n` venues to be retrieved from `r total_pages`.

```{r}
for(i in 0:(total_pages-1)){
  response <- GET(url = "https://app.ticketmaster.com/discovery/v2/venues",
    query = list(apikey = tm_api_key,
                 countryCode = country_code,
                 size = page_size,
                 page = i,
                 loacle = '*'))
  content_json <- content(response, as = "text")
  content <- fromJSON(content_json)
  content_embedded <- content[["_embedded"]]
  # assign(paste0("venues",i), content_embedded[["venues"]])
  venues <- content_embedded[["venues"]]
  
  # as long as we have not reached the last page, we insert data frames 
  # with 100 cases into venue_data_complete
  if(i < (total_pages-1)){
    venue_data["name"] <- venues[["name"]]
    venue_data["postalCode"] <- venues[["postalCode"]]
    venue_data["url"] <- venues[["url"]]
    venue_data["city"] <- venues$city[["name"]]
    venue_data["address"] <- venues$address[["line1"]]
    venue_data["longitude"] <- as.double(venues$location[["longitude"]])
    venue_data["latitude"] <- as.double(venues$location[["latitude"]])
    venue_data <- venue_data %>% 
      select(name, city, postalCode, address, url, longitude, latitude)
    venue_data_complete[(i*page_size + 1) : ((i+1) * page_size), ] <- venue_data
    # print(paste("Page:", i+1, "| Progress:", scales::percent(i/total_pages), sep = " "))
    Sys.sleep(2/5)
  # as soon as we reach the last page, the number of rows to be inserted into
  # venue_data_complete reduces to n %% 100 (the remainder of the division of
  # the total number of elements and the page size)
  } else{
    venue_data <- data.frame(
      name = character(n%%page_size),
      city = character(n%%page_size),
      postalCode = character(n%%page_size),
      address = character(n%%page_size),
      url = character(n%%page_size),
      longitude = numeric(n%%page_size),
      latitude = numeric(n%%page_size),
      stringsAsFactors = FALSE
      )
    
    venue_data["name"] <- venues[["name"]]
    venue_data["postalCode"] <- venues[["postalCode"]]
    venue_data["url"] <- venues[["url"]]
    venue_data["city"] <- venues$city[["name"]]
    venue_data["address"] <- venues$address[["line1"]]
    venue_data["longitude"] <- as.double(venues$location[["longitude"]])
    venue_data["latitude"] <- as.double(venues$location[["latitude"]])
    venue_data <- venue_data %>% 
      select(name, city, postalCode, address, url, longitude, latitude)
    venue_data_complete[(i*page_size + 1) : (i*page_size + n%%page_size), ] <- venue_data
    # print("final iteration")
    Sys.sleep(2/5)
  }
}

```


```{r}
venue_data_complete %>% 
  glimpse()
```


```{r}
# max_lat_be <- 51.4570
# min_lat_be <- 49.8209
# min_long_be <- 2.5804
# max_long_be <- 6.4033

max_lat_uk <- 60.85
min_lat_uk <- 49.85
min_long_uk <- -13.683333
max_long_uk <- 1.766667

venue_data_complete <- venue_data_complete %>% 
  filter(longitude < max_long_uk, longitude > min_long_uk,
         latitude < max_lat_uk, latitude > min_lat_uk)
```


```{r}
ggplot() +
  geom_polygon(
    aes(x = long, y = lat, group = group), data = map_data("world", region = "GB"),
    fill = "gray90",color = "black") +
  geom_point(data = venue_data_complete, aes(longitude, latitude), 
             color = "royalblue3",
             alpha = 0.5) +
  theme_void() + coord_quickmap() +
  labs(title = paste("Event locations across ", country_code), caption = "Source: ticketmaster.com") +
  theme(title = element_text(size=8, face='bold'),
    plot.caption = element_text(face = "italic")) 
```
